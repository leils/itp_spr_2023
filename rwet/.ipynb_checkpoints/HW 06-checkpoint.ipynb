{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac1f659",
   "metadata": {},
   "source": [
    "# Homework 06 - The Final \n",
    "\n",
    "Updating the work from Hw5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716a840",
   "metadata": {},
   "source": [
    "---\n",
    "# Loading transformers\n",
    "Just doing the basics. Copied from Homework 05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dbd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b2e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb86503",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4cae1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leiachang/opt/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/leiachang/opt/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once, I started to walk around the city to see if this was what you wanted. I started to walk around the city to see if this was what you wanted. I started to walk around the city to see if this was what you wanted.\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"Once, I started to walk\")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a37bd8",
   "metadata": {},
   "source": [
    "---\n",
    "# Fine Tuning with Peter Pan \n",
    "I think I want to fine-tune this model to fit something a little closer to home. I have a deep love for Peter Pan, so I downloaded the text from Project Gutenberg to work with. Peter Pan (as troublesome as some of the story is) holds themes of everlasting childhood, love for wonder, and imagination. \n",
    "\n",
    "File sources are: \n",
    "```\n",
    "\"sources/peter-pan.txt\"\n",
    "```\n",
    "\n",
    "At first I created a truncated text to work with (seen in the cell below), but I realized that the majority of the more interesting, descriptive language occurs later in the text. I decided to try to train using the full text instead, which definitely took longer, but still only about 4 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ccc393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a truncated peter-pan text to work with\n",
    "with open(\"sources/truncated-peter-pan.txt\", \"w\") as fh:\n",
    "    fh.write(open(\"sources/peter-pan.txt\").read()[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8a4542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: datasets in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: responses<0.19 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: multiprocess in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: xxhash in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: packaging in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: filelock in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f806b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e300b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/Users/leiachang/.cache/huggingface/datasets/text/default-33e0c349e4d6c2f1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac4031b766f4e239eb280a986d69aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = datasets.load_dataset('text', data_files=\"sources/peter-pan.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a659267b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_training_data = training_data.map(\n",
    "    lambda x: tokenizer(x['text']),\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be870bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 64\n",
    "# magic from https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_training_data = tokenized_training_data.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9244ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e2c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  train_dataset=lm_training_data['train'],\n",
    "                  args=TrainingArguments(\n",
    "                      output_dir='distilgpt2-finetune-peter-pan',\n",
    "                      num_train_epochs=1,\n",
    "                      do_train=True,\n",
    "                      do_eval=False\n",
    "                  ),\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e454bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leiachang/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 03:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=129, training_loss=4.0501832000969, metrics={'train_runtime': 184.2312, 'train_samples_per_second': 5.602, 'train_steps_per_second': 0.7, 'total_flos': 16853640413184.0, 'train_loss': 4.0501832000969, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d45ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb084a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It's time to rest.         It is the last time that anyone is ever gone and never has been there. I hope you will.”“““”“”“Not really,”“”” said St. John. “I wish you could go back, but I say a little more.”“No,” said Mrs.“Yes, no.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"It's time to rest.  \", max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d9d3b",
   "metadata": {},
   "source": [
    "---\n",
    "# Using the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97283e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am running away. I am running away, and \"\n",
    "generated = []\n",
    "\n",
    "for x in range(5): \n",
    "    generated.append(generator(prompt, temperature = 2.0, max_length = 50)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d55f3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am running away. I am running away, and Â will tell his wife to stay home until the night are now. She is to meet I amthe queen, in an evensong!‭This woman lives to have him up.\n",
      "I am running away. I am running away, and  Peter’nodd and I have now come; the wind came again onthat I hope I won´get rid you of this wonderful man if she is not so ill-happiness\n",
      "I am running away. I am running away, and _____the moon is now floating. This, again though, would be an event. On any event there would be a fly; they may well play for it at least two-three on\n",
      "I am running away. I am running away, and ~~”and he went in.‡I came for all. It was here and she left it again at once when they made up.She ran in tothe table by hand\n",
      "I am running away. I am running away, and ____Bubbles will grow by night.”Scoache and I have also grown by him by his breast, I suppose‣we did it after night, no one wants\n"
     ]
    }
   ],
   "source": [
    "for text in generated: \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e9c90d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_k_100 = []\n",
    "\n",
    "for x in range(5): \n",
    "    generated_k_100.append(generator(prompt, top_k = 100, temperature = 2.0, max_length = 50)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb35ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am running away. I am running away, and ___________________________ As night goes by to endwe will again followas she went for supper. Her life must be my last bed.I ran out without fear all the fairy in charge againin\n",
      "I am running away. I am running away, and iced upand“there’s your song:The End|tootty—get that he isn‵-yann  and that's the sort one songwriters\n",
      "I am running away. I am running away, and _______ can’er wantme left and get so angry that I try it once the children stop fighting each other and play it by a long, strong breath that at once weaspers\n",
      "I am running away. I am running away, and ÂYou seem very sure Mr Perry now and I cannot walk through. Let  take you to the spot of it firsttime, no problem, and soll do as he lay home\n",
      "I am running away. I am running away, and ircasmai is among others it was my very own work: from two-hour distancetoThebenequent has stood at, in a darkdark seen tully which lies\n"
     ]
    }
   ],
   "source": [
    "for text in generated_k_100: \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "935e6a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_k_500 = []\n",
    "\n",
    "for x in range(5): \n",
    "    generated_k_500.append(generator(prompt, top_k = 500, temperature = 2.0, max_length = 50)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2256417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am running away. I am running away, and ! That is good still: many or from the balcony, next Monday, it hardly may grow rich; but keep trying to pull apart first for whereletfor to kiss It then\n",
      "I am running away. I am running away, and ....................... as Dinkhouse wou had said Wier now --suddenlyB and they shall pass Jane has hurried home by-- under his powerDylan‡baring him;gone\n",
      "I am running away. I am running away, and ****************I WANT TER’ with ALLERSON who let Sarah keep, and amforth upon her hat leaves besidegably my side;what remain amoved this parting of story at it length\n",
      "I am running away. I am running away, and  Hook began!Father what dair answered quite alarm by once weft forward for much the long the nade waitagrosethere put where Hoomin should've taught, himselfflating\n",
      "I am running away. I am running away, and ~~ o of, was watching him cheering me with love.Would that fright broke easilyie quiet but let Tika run afterwards carrying?was the three eldest citizens all feeling grief slowlyonme\n"
     ]
    }
   ],
   "source": [
    "for text in generated_k_500: \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdd417",
   "metadata": {},
   "source": [
    "--- \n",
    "# Adding Formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c45e64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e13b8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_inserted = [\"\\n\", \"          \", \"\\n\\n\"]\n",
    "generated_02 = generated.copy()\n",
    "generated_02_spaced = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces(text): \n",
    "    words = text.split()\n",
    "    maxIndex = len(words)\n",
    "    chosenIndexes = random.sample(range(0, maxIndex), 5)\n",
    "    for i in chosenIndexes: \n",
    "        words.insert(i, random.choce(to_be_inserted))\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad578286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 10, 8, 3, 24]\n",
      "[14, 13, 1, 6, 11]\n",
      "[13, 17, 35, 8, 18]\n",
      "[38, 15, 10, 19, 35]\n",
      "[7, 29, 41, 9, 31]\n"
     ]
    }
   ],
   "source": [
    "for text in generated_02: \n",
    "    words = text.split()\n",
    "    maxIndex = len(words)\n",
    "    chosenIndexes = random.sample(range(0, maxIndex), 5)\n",
    "    print(chosenIndexes)\n",
    "    for i in chosenIndexes: \n",
    "        words.insert(i, random.choice(to_be_inserted))\n",
    "    generated_02_spaced.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56ec7587",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ran            \n",
      "\n",
      " away. I ran, and icky, \n",
      " and I \n",
      "\n",
      " went.“I’ll tell you,” he said, “I’ll tell you,” and he was \n",
      "\n",
      " so glad that he was not so\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I \n",
      "\n",
      " ran away. I ran,            and ________.”“Oh, dear,” he \n",
      " said, “we have to            go,� \n",
      "\n",
      " visas.”“No,” she said, “not to tell\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I ran away. I ran, and They had \n",
      " the light to tell you—thoughthe            stars in the \n",
      " \n",
      "\n",
      " sky dimened—and they could be up with you if they were up,but you could not talk; your            little mother had\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I ran away. I ran, and ____________”I went at it            wondering if he were dying. \n",
      "\n",
      " There is \n",
      " only one one who knows whom it is all his age; indeed one who was \n",
      " a girl, it became one. Wendy \n",
      " liked\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I ran away. I ran, and ~~ \n",
      "\n",
      " he \n",
      "\n",
      " was dead because John felt him, I found a home near Peterhouse on the river to put on one bedand                       let them fly where ~~ I came last winter to see \n",
      "\n",
      " him and see my\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in generated_02_spaced: \n",
    "    print(text)\n",
    "    print(\"\\n\\n\\n   -------------------------------------------- \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f06ad6",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Output \n",
    "\n",
    "\n",
    "```\n",
    "I ran            \n",
    "\n",
    " away. I ran, and icky, \n",
    " and I \n",
    "\n",
    " went.“I’ll tell you,” he said, “I’ll tell you,” and he was \n",
    "\n",
    " so glad that he was not so\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I \n",
    "\n",
    " ran away. I ran,            and ________.”“Oh, dear,” he \n",
    " said, “we have to            go,� \n",
    "\n",
    " visas.”“No,” she said, “not to tell\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I ran away. I ran, and They had \n",
    " the light to tell you—thoughthe            stars in the \n",
    " \n",
    "\n",
    " sky dimened—and they could be up with you if they were up,but you could not talk; your            little mother had\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I ran away. I ran, and ____________”I went at it            wondering if he were dying. \n",
    "\n",
    " There is \n",
    " only one one who knows whom it is all his age; indeed one who was \n",
    " a girl, it became one. Wendy \n",
    " liked\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I ran away. I ran, and ~~ \n",
    "\n",
    " he \n",
    "\n",
    " was dead because John felt him, I found a home near Peterhouse on the river to put on one bedand                       let them fly where ~~ I came last winter to see \n",
    "\n",
    " him and see my\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "```\n",
    "\n",
    "I am in love with this iteration's output, if not the whole piece. This reminds me of a book called [Death in Her Hands by Ottessa Moshfegh](https://www.newyorker.com/books/page-turner/ottessa-moshfeghs-death-in-her-hands-is-a-new-kind-of-murder-mystery), which a lovely friend lent me last summer and it left me in a TIZZY. In that book, a women is (perhaps) going more and more insane, or just dealing with more and more (let me not spoil it). Here, it feels like the series of sets are slowly coming up to the understanding and processing that someone has died, and that the author had to run away because of it. \n",
    "\n",
    "Applying the \"author\" to this feels almost like folly. I didn't write most of it, I left it at the hands of a language model. I want to lay my hands on this more deeply, but I'm not sure how yet. Perhaps I'd add more of my own words to the mix, or to run it over and over and over, re-structuring the poems on my own. I'm deeply intrigued by the process taken by David Jhave Johnston in [Rerites](http://glia.ca/rerites/), where the poet edited the generated output deeply and ritualistically every day. I may wish to perform something similar here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844f588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
