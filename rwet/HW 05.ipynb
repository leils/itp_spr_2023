{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac1f659",
   "metadata": {},
   "source": [
    "# Homework 05 \n",
    "\n",
    "> Use predictive models to generate text: either a Markov chain or a neural network, or both. How does your choice of source text affect the output? Try combining predictive text with other methods we’ve used for analyzing and generating text: use neural network-generated text to fill Tracery templates, or train a Markov model on the output of parsing parts of speech from a text, or some other combination. What works and what doesn’t? How does neural network-generated text “feel” different from Markov-generated text? How does the length of the n-gram and the unit of the n-gram affect the quality of the output?\n",
    "\n",
    "So, what do I have to work with? \n",
    "- Markov Chains \n",
    "- Neural Networks \n",
    "- Books from Project Gutenberg \n",
    "- Personal Writing \n",
    "- Tracery\n",
    "- NLP via SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716a840",
   "metadata": {},
   "source": [
    "---\n",
    "# Loading transformers\n",
    "Just doing the basics, directly copied from Allison's examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dbd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b2e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb86503",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4cae1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once, I started to walk with a fellow woman. It was weird, scary, uncomfortable, dangerous. It was very awkward and it was terrifying to me. I had just been on the bus.\n",
      "\n",
      "You are like a child.\n",
      "My\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"Once, I started to walk\")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a37bd8",
   "metadata": {},
   "source": [
    "---\n",
    "# Fine Tuning with Peter Pan \n",
    "I think I want to fine-tune this model to fit something a little closer to home. I have a deep love for Peter Pan, so I downloaded the text from Project Gutenberg to work with. Peter Pan (as troublesome as some of the story is) holds themes of everlasting childhood, love for wonder, and imagination. \n",
    "\n",
    "File sources are: \n",
    "```\n",
    "\"sources/peter-pan.txt\"\n",
    "```\n",
    "\n",
    "At first I created a truncated text to work with (seen in the cell below), but I realized that the majority of the more interesting, descriptive language occurs later in the text. I decided to try to train using the full text instead, which definitely took longer, but still only about 4 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18ccc393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a truncated peter-pan text to work with\n",
    "with open(\"sources/truncated-peter-pan.txt\", \"w\") as fh:\n",
    "    fh.write(open(\"sources/peter-pan.txt\").read()[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8a4542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-macosx_10_14_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-macosx_10_9_x86_64.whl (360 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: filelock in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leiachang/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f806b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22e300b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/leiachang/.cache/huggingface/datasets/text/default-33e0c349e4d6c2f1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f20d645d85d4b9ea86d68da8ea49462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd7e5dfb51049ee92346530dc520451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/leiachang/.cache/huggingface/datasets/text/default-33e0c349e4d6c2f1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d9d0c24e0c4b1da4f2aa7629e547d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = datasets.load_dataset('text', data_files=\"sources/peter-pan.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a659267b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_training_data = training_data.map(\n",
    "    lambda x: tokenizer(x['text']),\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be870bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6261 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 64\n",
    "# magic from https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_training_data = tokenized_training_data.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9244ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52e2c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  train_dataset=lm_training_data['train'],\n",
    "                  args=TrainingArguments(\n",
    "                      output_dir='distilgpt2-finetune-peter-pan',\n",
    "                      num_train_epochs=1,\n",
    "                      do_train=True,\n",
    "                      do_eval=False\n",
    "                  ),\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e454bacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 03:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=129, training_loss=3.843318968780281, metrics={'train_runtime': 198.4606, 'train_samples_per_second': 5.2, 'train_steps_per_second': 0.65, 'total_flos': 16853640413184.0, 'train_loss': 3.843318968780281, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d45ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb084a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I have no thoughts _____—he gave us. It is the last time he talked,” and now we are a little surprised. Wendy is a big-headed figure at your door.“Are she still crying?” he whispered thoughtfully.“I am going to be quite sure.”“I don’t have any hope.”She sat upand down on the floor when she spoke.“Yes,” said Peter to'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"I have no thoughts \", max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53620248",
   "metadata": {},
   "source": [
    "---\n",
    "# Great! Now what? \n",
    "I have a language model, tweaked to generate text based on Peter Pan by J.M. Barre. Here I attempt to use this language model to create some new works, with prompts based on some other sources. I take some quotes from the stories that I've gathered with my thesis test-runs, and use those as prompts to try and create some sort of juxtoposition between what was originally written and what it is \"reimagined\" in our fairy-tale land. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39f8f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_01 = \"I usually call my mother with bad news, or my friend Joe. When I want to tell them the worries I have, I know theyll listen with care and call me on my bullshit whenever I need them to. And they'll take a moment to mourn with me\"\n",
    "prompt_01 = \"I usually call my mother with bad news, \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cffb985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I usually call my mother with bad news,    ;and she was not my mother. Of course, but you know her, and you have to tell her that at the last moment. Which is very good. It would be almost impossible for the boys to change minds or the feeling that their faces had gone out of their minds to make them proud, and it was so that they thoughtfor a moment that they should have lost their minds. But they did not. In a moment when\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I usually call my mother with bad news,    _______________________________________________________.”“Yes,” she answered, “we will be very pleased to welcome you.”“It’s time to get back again,” he said, “what do you have to do?”“Then, we will go to school in the evening by night.“O’HO.”“So, where,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I usually call my mother with bad news,      , just in case she would like it to go home.” She will not be allowed to tell you what happened to Tootles, because there is the most fatal lesson to take—and to explain it in her simple language!”“Wendy’s mother said, ‘Hinde your bed!“What are you fussing about?’Then she looked at it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I usually call my mother with bad news,                                                                                           \n",
      "I usually call my mother with bad news, ick in the night, and I have no other person to call. But Peter is the only man in the world who knows the secret.He is always trying to persuade me. To all, who knows, but Peter is the only friend by nature.He is always trying to persuade me with bad news. Peter is the man of course. But he is the only person in the world who knows all the secrets.”“When I\n"
     ]
    }
   ],
   "source": [
    "for x in range(5): \n",
    "    print(generator(prompt_01, max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4d72b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6f3652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I usually call my mother with bad news,   they can be told it is a game. When the mother is told it is a game, it is an empty box. For some time she is not allowed to talk, but sometimes she can. She is not allowed to be, and her time is spent, thinking with the children her hand about it. Ofthis she is not always safe enough to call one another, and there is no one to whom she may give it a chance to\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb4dcc",
   "metadata": {},
   "source": [
    "The original: \n",
    "```\n",
    "I usually call my mother with bad news, or my friend Joe. When I want to tell them the worries I have, I know theyll listen with care and call me on my bullshit whenever I need them to. And they'll take a moment to mourn with me\n",
    "```\n",
    "\n",
    "And the generated: \n",
    "\n",
    "```\n",
    "I usually call my mother with bad news,   they can be told it is a game. When the mother is told it is a game, it is an empty box. For some time she is not allowed to talk, but sometimes she can. She is not allowed to be, and her time is spent, thinking with the children her hand about it. Ofthis she is not always safe enough to call one another, and there is no one to whom she may give it a chance to\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab7a3104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a small box I got from India _______________  ________ ________  ________ ________ ________________   ________       ________   ________________  ________                                                         \n"
     ]
    }
   ],
   "source": [
    "b = generator(\"I have a small box I got from India \", max_length=100)[0]['generated_text']\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86314e59",
   "metadata": {},
   "source": [
    "That's ... strange. I wonder if there's just a number of breaks in there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1ebf431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a small box I got from India !!!!!!!Dice not-so large-wise in it!It says a thing all right: we all know one another and all love to say the greatest love that you give andtell with your hearts like yourest work. Which may, from it shall surely know not too more!Tightly it goes, let our best friends call it.Oh! they do. Howl really the heart must grow! TIGHTDATE FOR LIFE!!!!\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a small box I got from India   so wellI“lovelyly-sounding call I never saw ~~~~ ~~~~~~~~__;Y a moment I am in. When no need come on, just because we did ********‏, it can turn some lovely colour against the tree leavesand now at some other lightening in that place I was walking without sightless—before they could hear theiroftoil; I shall only do this once every night!The house\n",
      "--------------------\n",
      "I have a small box I got from India  this was my only thing till then whichI made it the same or of little black colour into, like the very large haters gave to TubbyDance in Tumpme!They thought in such things wheninvisible shadow, theyhad got them very nice at the moment till quite often that of the white in blue who didof that time byadoe to them from outside the window was found just to leave me for good at some\n"
     ]
    }
   ],
   "source": [
    "for x in range(3): \n",
    "    print(\"--------------------\")\n",
    "    print(generator(\"I have a small box I got from India \", temperature=3.0, max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2c877",
   "metadata": {},
   "source": [
    "Original: \n",
    "```\n",
    "I have a small box I got from India in which I keep little notes and cards and tokens from people I love. I've moved a lot, but this box stays with me wherever I go. It's the one thing I'd grab if there were a fire.\n",
    "```\n",
    "\n",
    "Generated: \n",
    "```\n",
    "I have a small box I got from India 〙 to a long distance, it was the only way to look. But my nose was blown open by the sun.”“I have a small box, I have a small box, I have a small box, and I have a small box, I have a small box, and I have a small box. I have a small box, and I have a small box.”“Where is my bag?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356451b",
   "metadata": {},
   "source": [
    "---\n",
    "# And now something more critical\n",
    "Alright, that was informative. The prompts are curious, but I'm not sure the prompts and the model really work well together. \n",
    "There's something here in the model/corpus/original text about fantasy, escapism, and running away. What can we do with that? \n",
    "\n",
    "Taking for example my last Homework attempt (where I created a poem that forgot itself as I ran it), I now try to create a poem that runs further and further afoot, or further and further consistent. The texts generated by the neural net are already nonsensical (while some sense can be made, it certainly does not read as if a person wrote it, or moreso it reads as if someone were stream-of-conscious writing but with little to no preconcieved idea of consistency or attempting to communicate something). So let's lean into that, rather than away from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1648b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I ran away. I ran, and \"\n",
    "generated = []\n",
    "temperatures = []\n",
    "temp = 0.1\n",
    "addTemp = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ef543b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for x in range (5): \n",
    "    generated.append(generator(prompt, temperature=temp, max_length=50)[0]['generated_text'])\n",
    "    temperatures.append(temp)\n",
    "    temp += addTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce5b3a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ran away. I ran, and icky, and I went.“I’ll tell you,” he said, “I’ll tell you,” and he was so glad that he was not so\n",
      "\n",
      "\n",
      "\n",
      "I ran away. I ran, and ________.”“Oh, dear,” he said, “we have to go,� visas.”“No,” she said, “not to tell\n",
      "\n",
      "\n",
      "\n",
      "I ran away. I ran, and    They had the light to tell you—thoughthe stars in the sky dimened—and they could be up with you if they were up,but you could not talk; your little mother had\n",
      "\n",
      "\n",
      "\n",
      "I ran away. I ran, and ____________”I went at it wondering if he were dying. There is only one one who knows whom it is all his age; indeed one who was a girl, it became one. Wendy liked\n",
      "\n",
      "\n",
      "\n",
      "I ran away. I ran, and ~~ he was dead because John felt him, I found a home near Peterhouse on the river to put on one bedand let them fly where ~~ I came last winter to see him and see my\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in generated: \n",
    "    print(text)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b8dee",
   "metadata": {},
   "source": [
    "That's kind of lovely. It gets a bit dark, with the dying and dead, and I love that. It's hard to read though, and I want to try and add some kind of breaks into it (and and curious spacing). Let's break this up into words, and add some spaces around it using random.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c45e64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e13b8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_inserted = [\"\\n\", \"          \", \"\\n\\n\"]\n",
    "generated_02 = generated.copy()\n",
    "generated_02_spaced = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad578286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 10, 8, 3, 24]\n",
      "[14, 13, 1, 6, 11]\n",
      "[13, 17, 35, 8, 18]\n",
      "[38, 15, 10, 19, 35]\n",
      "[7, 29, 41, 9, 31]\n"
     ]
    }
   ],
   "source": [
    "for text in generated_02: \n",
    "    words = text.split()\n",
    "    maxIndex = len(words)\n",
    "    chosenIndexes = random.sample(range(0, maxIndex), 5)\n",
    "    print(chosenIndexes)\n",
    "    for i in chosenIndexes: \n",
    "        words.insert(i, random.choice(to_be_inserted))\n",
    "    generated_02_spaced.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56ec7587",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ran            \n",
      "\n",
      " away. I ran, and icky, \n",
      " and I \n",
      "\n",
      " went.“I’ll tell you,” he said, “I’ll tell you,” and he was \n",
      "\n",
      " so glad that he was not so\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I \n",
      "\n",
      " ran away. I ran,            and ________.”“Oh, dear,” he \n",
      " said, “we have to            go,� \n",
      "\n",
      " visas.”“No,” she said, “not to tell\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I ran away. I ran, and They had \n",
      " the light to tell you—thoughthe            stars in the \n",
      " \n",
      "\n",
      " sky dimened—and they could be up with you if they were up,but you could not talk; your            little mother had\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I ran away. I ran, and ____________”I went at it            wondering if he were dying. \n",
      "\n",
      " There is \n",
      " only one one who knows whom it is all his age; indeed one who was \n",
      " a girl, it became one. Wendy \n",
      " liked\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n",
      "I ran away. I ran, and ~~ \n",
      "\n",
      " he \n",
      "\n",
      " was dead because John felt him, I found a home near Peterhouse on the river to put on one bedand                       let them fly where ~~ I came last winter to see \n",
      "\n",
      " him and see my\n",
      "\n",
      "\n",
      "\n",
      "   -------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in generated_02_spaced: \n",
    "    print(text)\n",
    "    print(\"\\n\\n\\n   -------------------------------------------- \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f06ad6",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Output \n",
    "\n",
    "\n",
    "```\n",
    "I ran            \n",
    "\n",
    " away. I ran, and icky, \n",
    " and I \n",
    "\n",
    " went.“I’ll tell you,” he said, “I’ll tell you,” and he was \n",
    "\n",
    " so glad that he was not so\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I \n",
    "\n",
    " ran away. I ran,            and ________.”“Oh, dear,” he \n",
    " said, “we have to            go,� \n",
    "\n",
    " visas.”“No,” she said, “not to tell\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I ran away. I ran, and They had \n",
    " the light to tell you—thoughthe            stars in the \n",
    " \n",
    "\n",
    " sky dimened—and they could be up with you if they were up,but you could not talk; your            little mother had\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I ran away. I ran, and ____________”I went at it            wondering if he were dying. \n",
    "\n",
    " There is \n",
    " only one one who knows whom it is all his age; indeed one who was \n",
    " a girl, it became one. Wendy \n",
    " liked\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "\n",
    "I ran away. I ran, and ~~ \n",
    "\n",
    " he \n",
    "\n",
    " was dead because John felt him, I found a home near Peterhouse on the river to put on one bedand                       let them fly where ~~ I came last winter to see \n",
    "\n",
    " him and see my\n",
    "\n",
    "\n",
    "\n",
    "   -------------------------------------------- \n",
    "```\n",
    "\n",
    "I am in love with this iteration's output, if not the whole piece. This reminds me of a book called [Death in Her Hands by Ottessa Moshfegh](https://www.newyorker.com/books/page-turner/ottessa-moshfeghs-death-in-her-hands-is-a-new-kind-of-murder-mystery), which a lovely friend lent me last summer and it left me in a TIZZY. In that book, a women is (perhaps) going more and more insane, or just dealing with more and more (let me not spoil it). Here, it feels like the series of sets are slowly coming up to the understanding and processing that someone has died, and that the author had to run away because of it. \n",
    "\n",
    "Applying the \"author\" to this feels almost like folly. I didn't write most of it, I left it at the hands of a language model. I want to lay my hands on this more deeply, but I'm not sure how yet. Perhaps I'd add more of my own words to the mix, or to run it over and over and over, re-structuring the poems on my own. I'm deeply intrigued by the process taken by David Jhave Johnston in [Rerites](http://glia.ca/rerites/), where the poet edited the generated output deeply and ritualistically every day. I may wish to perform something similar here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844f588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
