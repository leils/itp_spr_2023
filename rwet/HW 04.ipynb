{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f2dca1",
   "metadata": {},
   "source": [
    "# It's HW 04 Time \n",
    "\n",
    "> The digital cut-up revisited. In assignment #2, the tools available to you for cutting up and rearranging texts relied only on information present in the character data itself. Since then, we’ve learned several methods for incorporating outside information concerning syntax (i.e. with spaCy) and semantics (i.e., word vectors) into what we “know” about a text in question. Adapt your original digital cut-up assignment, making use of one of these new sources of information. What new aesthetic possibilities are made available if the unit of the cut-up can be a type of syntactic unit (instead of words, lines, characters), and if stretches of text can be algorithmically selected not at random, but based on their meaning?\n",
    "\n",
    "Tools I can work with: \n",
    "- Tracery grammers \n",
    "- SpaCy NLP \n",
    "- word vectors \n",
    "\n",
    "Text I can work with: \n",
    "- My interview transcripts \n",
    "- Corpora Project \n",
    "- [Original HW2, a cutup around memory](https://github.com/leils/itp_spr_2023/blob/main/rwet/02_mixed_lines/mixed-lines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619527c",
   "metadata": {},
   "source": [
    "## Loading SpaCy and my source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61918c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c32c0",
   "metadata": {},
   "source": [
    "Starting with the poem I wrote last summer, `Retrieval-induced distortion`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675aeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = open(\"sources/retrieval.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402511d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-induced distortion;\n",
      "a phenomena in memory \n",
      "where a memory, sweet or sour, \n",
      "changes as you remember it. \n",
      "\n",
      "When you trace the line of your memories\n",
      "Tug the string, pull them from the ether\n",
      "lay them out on the deck \n",
      "and they mutate, stretched thin by your tugging \n",
      "smudged with your fingerprints \n",
      "\n",
      "I remember a day as a toddler\n",
      "a wood-burning stove, warming the house\n",
      "and a frozen winter beyond glass sliding doors. \n",
      "only this memory reads \n",
      "back and forth \n",
      "from a VHS tape. \n"
     ]
    }
   ],
   "source": [
    "print(retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbad2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_doc = nlp(retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b6d29",
   "metadata": {},
   "source": [
    "Because the [original poem](https://leils.github.io/telescopic-poems/21_26-01.html) was written to reflect how our recalling of memories can change them, I wanted to emulate that again here. My goal: to chance the poem more and more, each time you \"recall\" it. I want to do this in a way that still is plausible (ie. doesn't just look like a randomizer was set upon all the words with no regard to the meaning). \n",
    "\n",
    "My idea:\n",
    "- re-writing each line so that it's less and less accurate each run, like every time it's read we're moving the meaning further and further away \n",
    "\n",
    "Basically, I'm attempting to: \n",
    "1. use spacy to understand what a word is (noun, verb, etc) \n",
    "2. Replace words with other words of their ilk (noun/verb/etc)  \n",
    "3. Do this more and more across runs.\n",
    "\n",
    "Questions that I think affect the concept of the piece: \n",
    "- Do I rewrite the original file across runs? Or the original \"loaded\" file? <--- (I chose not to do this for repeatability's sake) \n",
    "- Do I add a small chance of removing words? <--- (I did this) \n",
    "\n",
    "I think I'm realizing how little I know about language here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ed8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "nouns = []\n",
    "for word in retrieval_doc: \n",
    "    if word.pos_ == \"VERB\":\n",
    "        verbs.append(word.text)\n",
    "    elif word.pos_ == \"NOUN\":\n",
    "        nouns.append(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8689d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['distortion', 'phenomena', 'memory', 'memory', 'changes', 'line', 'memories', 'string', 'ether', 'deck', 'tugging', 'fingerprints', 'day', 'toddler', 'wood', 'stove', 'house', 'winter', 'glass', 'doors', 'memory', 'tape']\n"
     ]
    }
   ],
   "source": [
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ef8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['induced', 'remember', 'trace', 'pull', 'lay', 'mutate', 'stretched', 'smudged', 'remember', 'burning', 'warming', 'sliding', 'reads']\n"
     ]
    }
   ],
   "source": [
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfd57dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_times = 0\n",
    "chance_to_swap = 0.03\n",
    "chance_to_forget = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cad74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_poem():\n",
    "    global chance_to_swap, chance_to_forget\n",
    "    print(\"Chance at faulty recall: \", chance_to_swap, \", \", chance_to_forget)\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    \n",
    "    word_to_print = \"\"\n",
    "    for word in retrieval_doc:\n",
    "        rolled = random.random()\n",
    "        if rolled <= chance_to_forget: \n",
    "            word_to_print = \" \" * len(word.text)\n",
    "        else: \n",
    "            if (word.pos_ == \"VERB\") and (rolled <= chance_to_swap):\n",
    "                word_to_print = random.choice(verbs)\n",
    "            elif (word.pos_ == \"NOUN\") and (rolled <= chance_to_swap):\n",
    "                word_to_print = random.choice(nouns)\n",
    "            else:\n",
    "                word_to_print = word.text\n",
    "            \n",
    "        print(word_to_print, end=\" \")\n",
    "        \n",
    "    if (chance_to_swap < 1):\n",
    "        chance_to_swap += .1\n",
    "    else:\n",
    "        chance_to_forget += .1\n",
    "            \n",
    "def reset():\n",
    "    global chance_to_swap, chance_to_forget\n",
    "    chance_to_swap = 0.03\n",
    "    chance_to_forget = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73db0d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance at faulty recall:  1.03 ,  0.9999999999999999\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 "
     ]
    }
   ],
   "source": [
    "recall_poem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afa0be",
   "metadata": {},
   "source": [
    "---\n",
    "## Some nice outcomes: \n",
    "Chance at faulty recall:  0.73 ,  0\n",
    "```\n",
    "Retrieval - induced distortion ; \n",
    " a doors in memory \n",
    " where a memory , sweet or sour , \n",
    " phenomena as you pull it . \n",
    "\n",
    " When you burning the memory of your phenomena \n",
    " Tug the memory , pull them from the tape \n",
    " lay them out on the tape \n",
    " and they mutate , reads thin by your fingerprints \n",
    " remember with your fingerprints \n",
    "\n",
    " I remember a line as a doors \n",
    " a wood - burning string , remember the tape \n",
    " and a frozen deck beyond glass warming doors . \n",
    " only this deck mutate \n",
    " back and forth \n",
    " from a VHS memory . \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d08ab",
   "metadata": {},
   "source": [
    "Chance at faulty recall:  1.03 ,  0.2\n",
    "```\n",
    "Retrieval - lay fingerprints ; \n",
    " a memory in memory \n",
    " where a glass , sweet    sour ,   day as              it . \n",
    "\n",
    " When you induced the      of your phenomena \n",
    " Tug the doors , reads them      the fingerprints   sliding them out on the string   and they stretched , trace thin by your deck \n",
    " lay           string \n",
    "\n",
    " I warming a phenomena as           \n",
    " a toddler - stretched phenomena , smudged the       \n",
    " and a        line beyond memories remember distortion   \n",
    " only this        mutate \n",
    " back             from a VHS doors . \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71fc87",
   "metadata": {},
   "source": [
    "I really, really liked this. These are really beautiful to me, and I love that they remix the internal contents of the poem. Because I'm swapping nouns with nouns and verbs with verbs, the poem still tends to make sense, until it very much so does not. In the end, there is nothing left of the poem. \n",
    "\n",
    "You can see a [video of the full run](https://youtu.be/g0GKooU_wUk) here. \n",
    "\n",
    "What would I like to do with this in the future? \n",
    "- This program does not actively destroy the original text, only its copy in memory. I like that, but I think it would be interesting to play with destroying the original text as well. \n",
    "- I wanted to swap the nouns and verbs with others from the Corpora project, but after looking at the JSON file, I felt unsatisfied with that word bank. I think I'd like to give this another word bank at some point. \n",
    "- I wonder what it might look like to \"mix\" memories through mixing poems. \n",
    "\n",
    "~ End of assignment. Below is another experiment. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d22c78",
   "metadata": {},
   "source": [
    "---\n",
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c06c3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_noun_chunks = list(retrieval_doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e83aa533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "------------\n",
      "Retrieval-induced distortion\n",
      "a day\n",
      "them\n",
      "Tug\n",
      "the deck\n",
      "a wood-burning stove\n",
      "the line\n",
      "the ether\n",
      "your memories\n",
      "them\n"
     ]
    }
   ],
   "source": [
    "print(len(ret_noun_chunks))\n",
    "print('------------')\n",
    "for n in random.sample(ret_noun_chunks, 10): print(n.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac1bdf",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Side experiments with interview transcripts\n",
    "\n",
    "I also wanted to test out some of these tools with the my interview transcripts. These particular transcripts have been adapted and edited from Descript, which intakes audio files and outputs transcripts. These transcripts are ... well, as accurate as you might expect from an audio transcription service. While I've edited them a bit, it seems like it might be as much work to edit them as to transcribe them myself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "829d7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_1 = open(\"sources/01.txt\").read()\n",
    "interview_2 = open(\"sources/02.txt\").read()\n",
    "interview_3 = open(\"sources/03.txt\").read()\n",
    "interview_4 = open(\"sources/04.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c376d0",
   "metadata": {},
   "source": [
    "---\n",
    "## Working with Transcripts\n",
    "In these interviews, I've replaced the names with \"Interviewee1\" and such. However, there's still a lot that you can glean from these interviews especially if you know about the group of people that these selected interviews come from, so it's a bit of an uncomfortable thing to work with. I'm trying to wrestle with this fact, while continuing to work with them. \n",
    "\n",
    "There are a couple of issues with the source text. For example, here's a list of \"People\" SpaCy pulled from interview 2: \n",
    "\n",
    "```\n",
    "[YouTuber,\n",
    " 00:08:00,\n",
    " I.,\n",
    " unnie,\n",
    " unnie,\n",
    " 00:17:00,\n",
    " Matt,\n",
    " Twinkie,\n",
    " 00:24:00,\n",
    " 00:30:00,\n",
    " 00:32:00,\n",
    " Dad,\n",
    " dnr,\n",
    " 00:39:00,\n",
    " 00:41:00,\n",
    " 00:46:00,\n",
    " Juwan,\n",
    " 00:47:00,\n",
    " Dora,\n",
    " Barbie]\n",
    "```\n",
    "\n",
    "I had timestamps in them, which definitely didn't work here. So, I figured out a bit of regex to find-and-replace remove them: `\\[0.:.*\\]`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc511b9",
   "metadata": {},
   "source": [
    "Task: Determine main characters, places, themes in the interview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c6f4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview1_doc = nlp(interview_1)\n",
    "interview2_doc = nlp(interview_2)\n",
    "interview3_doc = nlp(interview_3)\n",
    "interview4_doc = nlp(interview_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b9124",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9cf41",
   "metadata": {},
   "source": [
    "Let's work with interivew 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "778d4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_01 = list(interview1_doc.ents)\n",
    "noun_chunks_01 = list(interview1_doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a15161b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Argentina, Miami, New York, 10 years, three, Argentina, Argentina, World War, Spanish, Italian]\n"
     ]
    }
   ],
   "source": [
    "print(entities_01[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d00d2",
   "metadata": {},
   "source": [
    "I want to take this list, and find the most commonly mentioned entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f6d643b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPE'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_01[2].label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c54210ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_01 = [e for e in entities_01 if e.label_ == \"PERSON\"]\n",
    "locations_01 = [e for e in entities_01 if e.label_ == \"LOC\"]\n",
    "gpe_01 = [e for e in entities_01 if e.label_ == \"GPE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f63c2676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[South Italy,\n",
       " Latin America,\n",
       " Americanness,\n",
       " South America,\n",
       " Americanness,\n",
       " Americanness,\n",
       " Americanness,\n",
       " South America,\n",
       " Caribbean,\n",
       " south America,\n",
       " Central America,\n",
       " Central America,\n",
       " Northeastern,\n",
       " South America]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5b53851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Miami, New York, Venezuela, Miami, the United States]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(gpe_01, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8963f7c",
   "metadata": {},
   "source": [
    "What is the difference between a location and a gpe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab356767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And then the kids who came after are like, fuck English, really? \n",
      "\n",
      "Yes.\n",
      "Hmm.\n",
      "You know?\n",
      "You don't?\n"
     ]
    }
   ],
   "source": [
    "sents_01 = [item.text for item in interview1_doc.sents]\n",
    "for s in random.sample(sents_01, 5): print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea54f08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e513bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_02 = list(interview2_doc.ents)\n",
    "noun_chunks_02 = list(interview2_doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0b0663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_02 = [e for e in entities_02 if e.label_ == \"PERSON\"]\n",
    "locations_02 = [e for e in entities_02 if e.label_ == \"LOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fd2feab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Americanness, the Bay Area, the Bay Area, Americanness, Americanness]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f7664",
   "metadata": {},
   "source": [
    "It's really interesting to me that this model recognizes 'Americanness' as a location. There's something SUPER intriguing about that to me, and I want to follow that further in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1598986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_02 = [item.text for item in interview2_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da211be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and he would say it in English and my dad would say like, reply in broken English, or like, go in Korean. \n",
      "\n",
      "And just like, cuz like for me, at least at this point in my life, I say that like, if I'm gonna talk shit about someone,  \n",
      "Interviewer: I'm okay saying that to their face. \n",
      "\n",
      "And.\n",
      "Like ittastes like dish water, but like to me, that tastes like home and that tastes like hours of effort by my grandma put into it. \n",
      "\n",
      "There's still a lot of like what physical beauty matters.\n"
     ]
    }
   ],
   "source": [
    "for s in random.sample(sents_02, 5): print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe1dd9",
   "metadata": {},
   "source": [
    "One output that felt like a poem: \n",
    "```\n",
    "yeah.\n",
    "But yeah, it's just.\n",
    "and I kind of don't process it properly.\n",
    "Speaking Korean.\n",
    "But yeah.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc5ca6",
   "metadata": {},
   "source": [
    "Random thought dump 3/21 \n",
    "- counting how much something is mentioned \n",
    "- swapping something in the middle of a mention (ie. if we swap sentences where they both mention \"Adam\") \n",
    "- grab descriptions from these interviews, place them on places??? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
